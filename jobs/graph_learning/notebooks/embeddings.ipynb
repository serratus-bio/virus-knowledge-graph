{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/envs/rnalab/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports and config for the notebook\n",
    "\n",
    "## Notebook config\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "%load_ext dotenv\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graphistry\n",
    "\n",
    "from datasources.neo4j import gds\n",
    "from queries import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<graphdatascience.graph_data_science.GraphDataScience object at 0x7f4a12cb81d0>\n"
     ]
    }
   ],
   "source": [
    "# Configs\n",
    "\n",
    "# graphistry.register(\n",
    "#     api=3,\n",
    "#     username=os.getenv('GRAPHISTRY_USERNAME'),\n",
    "#     password=os.getenv('GRAPHISTRY_PASSWORD'),\n",
    "# )\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "EMBEDDINGS_DIR = '/mnt/embeddings/'\n",
    "\n",
    "print(gds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "\n",
    "- Create Tissue projection (random generate initial embeddings)\n",
    "- Create Taxon projection (use rank as initial feature?)\n",
    "\n",
    "- Create heterogenous projection from dataset?\n",
    "- run memory estimates for hashgnn and fastRP using hetero projection and \n",
    "\n",
    "\n",
    "- HashGNN_homogenous, HashGNN_heterogenous\n",
    "- FastRP_homogenous, FastRP_heterogenous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXON_PROJECTION_NAME = 'taxon-graph'\n",
    "TISSUE_PROJECTION_NAME = 'tissue-graph'\n",
    "HOMOGENOUS_PROJECTION_NAME = 'homogenous-graph'\n",
    "HETERO_PROJECTION_NAME = 'hetero-graph'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_homogenous_projection():\n",
    "    projection = gds.graph.project(\n",
    "        graph_name=HOMOGENOUS_PROJECTION_NAME,\n",
    "        node_spec=[\n",
    "            'Taxon',\n",
    "            'Tissue',\n",
    "            'SOTU',\n",
    "        ],\n",
    "        relationship_spec={\n",
    "            'HAS_PARENT': {'orientation': 'UNDIRECTED'},\n",
    "            'SEQUENCE_ALIGNMENT': {'orientation': 'UNDIRECTED'},\n",
    "        },\n",
    "    )\n",
    "    return projection\n",
    "\n",
    "\n",
    "# TODO: Use dataset projection\n",
    "def get_heterogenous_projection():\n",
    "    projection = gds.graph.project(\n",
    "        graph_name=HETERO_PROJECTION_NAME,\n",
    "        node_spec=[\n",
    "            'Taxon',\n",
    "            'Tissue',\n",
    "            'SOTU',\n",
    "        ],\n",
    "        relationship_spec={\n",
    "            'HAS_PARENT': {'orientation': 'UNDIRECTED'},\n",
    "            'SEQUENCE_ALIGNMENT': {'orientation': 'UNDIRECTED'},\n",
    "        },\n",
    "    )\n",
    "    return projection\n",
    "\n",
    "\n",
    "def get_taxon_projection():\n",
    "    projection = gds.graph.project(\n",
    "        graph_name=TAXON_PROJECTION_NAME,\n",
    "        node_spec=[\n",
    "            'Taxon',\n",
    "        ],\n",
    "        relationship_spec={\n",
    "            'HAS_PARENT': {'orientation': 'UNDIRECTED'},\n",
    "        },\n",
    "    )\n",
    "    return projection\n",
    "\n",
    "def get_tissue_projection():\n",
    "    projection = gds.graph.project(\n",
    "        graph_name=TISSUE_PROJECTION_NAME,\n",
    "        node_spec=['Tissue'],\n",
    "        relationship_spec={\n",
    "            'HAS_PARENT': {'orientation': 'UNDIRECTED'},\n",
    "        },\n",
    "    )\n",
    "    return projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = get_homogenous_projection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requiredMemory                                [1338 MiB ... 12865 MiB]\n",
       "treeView             Memory Estimation: [1338 MiB ... 12865 MiB]\\n|...\n",
       "mapView              {'memoryUsage': '[1338 MiB ... 12865 MiB]', 'n...\n",
       "bytesMin                                                    1403659432\n",
       "bytesMax                                                   13490131432\n",
       "nodeCount                                                      3021618\n",
       "relationshipCount                                             57702540\n",
       "heapPercentageMin                                                  0.1\n",
       "heapPercentageMax                                                  0.6\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/#algorithms-embeddings-hashgnn-syntax\n",
    "# https://github.com/neo4j/graph-data-science-client/blob/main/examples/heterogeneous-node-classification-with-hashgnn.ipynb#L18\n",
    "\n",
    "# one may try to set embeddingDensity to 128, 256, 512, or roughly 25%-50% of the embedding dimension, i.e. the number of binary features.\n",
    "\n",
    "gds.hashgnn.stream.estimate(\n",
    "    G=gds.graph.get(HOMOGENOUS_PROJECTION_NAME),\n",
    "    nodeLabels=['Taxon', 'Tissue', 'SOTU'],\n",
    "    relationshipTypes=['HAS_PARENT', 'SEQUENCE_ALIGNMENT'],\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    generateFeatures={\n",
    "        'dimension': 512, # dimension of the embedding vector\n",
    "        'densityLevel': 2, # number of initial values equalling 1\n",
    "    },\n",
    "    iterations=10, # maximum number of hops\n",
    "    embeddingDensity=256,\n",
    "    neighborInfluence=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HashGNN:  48%|████▊     | 47.58/100 [1:53:11<2:18:41, 158.75s/%]"
     ]
    }
   ],
   "source": [
    "filename = 'HashGNN_homogenous.csv'\n",
    "df = gds.hashgnn.stream(\n",
    "    G=gds.graph.get(HOMOGENOUS_PROJECTION_NAME),\n",
    "    nodeLabels=['Taxon', 'Tissue', 'SOTU'],\n",
    "    relationshipTypes=['HAS_PARENT', 'SEQUENCE_ALIGNMENT'],\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    generateFeatures={\n",
    "        'dimension': 512, # dimension of the embedding vector\n",
    "        'densityLevel': 2, # number of initial values equalling 1\n",
    "    },\n",
    "    iterations=10, # maximum number of hops\n",
    "    embeddingDensity=256,\n",
    "    neighborInfluence=1.0,\n",
    ")\n",
    "\n",
    "df.to_csv(EMBEDDING_DIR + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.fastrp.stream.estimate(\n",
    "    G=gds.graph.get(HOMOGENOUS_PROJECTION_NAME),\n",
    "    nodeLabels=['Taxon', 'Tissue', 'SOTU'],\n",
    "    relationshipTypes=['HAS_PARENT', 'SEQUENCE_ALIGNMENT'],\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    embeddingDimension=256,\n",
    "    relationshipWeightProperty=\"weight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'FastRP_homogenous.csv'\n",
    "df = gds.fastrp.stream(\n",
    "    G=gds.graph.get(HOMOGENOUS_PROJECTION_NAME),\n",
    "    nodeLabels=['Taxon', 'Tissue', 'SOTU'],\n",
    "    relationshipTypes=['HAS_PARENT', 'SEQUENCE_ALIGNMENT'],\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    embeddingDimension=256,\n",
    "    relationshipWeightProperty='weight',\n",
    ")\n",
    "df.to_csv(EMBEDDING_DIR + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading local file:  /mnt/graphdata/query_cache/neo4j/sotu_nodes.csv\n",
      "Reading local file:  /mnt/graphdata/query_cache/neo4j/taxon_nodes.csv\n",
      "Reading local file:  /mnt/graphdata/query_cache/neo4j/tissue_nodes.csv\n",
      "Reading local file:  /mnt/graphdata/query_cache/neo4j/sotu_has_host_stat_edges.csv\n",
      "Reading local file:  /mnt/graphdata/query_cache/neo4j/taxon_has_parent_edges.csv\n",
      "Reading local file:  /mnt/graphdata/query_cache/neo4j/sotu_sequence_alignment_edges.csv\n",
      "Reading local file:  /mnt/graphdata/query_cache/neo4j/sotu_has_inferred_taxon_edges.csv\n",
      "Reading local file:  /mnt/graphdata/query_cache/neo4j/sotu_has_tissue_metadata_edges.csv\n"
     ]
    }
   ],
   "source": [
    "from queries import feature_queries\n",
    "from config.base import (\n",
    "    DIR_CFG,\n",
    "    MODEL_CFG,\n",
    "    DATASET_CFG,\n",
    ")\n",
    "\n",
    "dir_name = '/mnt/graphdata/query_cache/neo4j/'\n",
    "\n",
    "\n",
    "dataset_cfg = DATASET_CFG\n",
    "nodes = feature_queries.get_all_node_features(\n",
    "    dir_name=dir_name,\n",
    "    dataset_cfg=dataset_cfg,\n",
    ")\n",
    "\n",
    "relationships = feature_queries.get_all_relationship_features(\n",
    "    dir_name=dir_name,\n",
    "    dataset_cfg=dataset_cfg,\n",
    ")\n",
    "\n",
    "undirected_relationship_types = list(\n",
    "    map(\n",
    "        (lambda cfg: cfg['TYPES'][0]),\n",
    "        dataset_cfg['REL_TYPES']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3021618, 3)\n",
      "(24096965, 4)\n",
      "incl-best-1_1\n"
     ]
    }
   ],
   "source": [
    "print(nodes.shape)\n",
    "print(relationships.shape)\n",
    "\n",
    "model_cfg = MODEL_CFG\n",
    "sampling_ratio = 1\n",
    "\n",
    "graph_name = \\\n",
    "        f\"{model_cfg['PROJECTION_NAME']}_{sampling_ratio}\"\n",
    "\n",
    "print(graph_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queries import pyg_queries, utils\n",
    "from config.base import MODEL_CFG, DATASET_CFG\n",
    "\n",
    "from queries.feature_queries import (\n",
    "    IdentityEncoder,\n",
    "    ListEncoder,\n",
    "    load_edge_tensor,\n",
    "    load_node_tensor,\n",
    ")\n",
    "from queries.utils import read_ddf_from_disk\n",
    "from models.models_v3 import Model\n",
    "from config.base import (\n",
    "    DIR_CFG,\n",
    "    MODEL_CFG,\n",
    "    DATASET_CFG,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "import torch\n",
    "from torch_geometric import seed_everything\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "seed_everything(MODEL_CFG['RANDOM_SEED'])\n",
    "\n",
    "class RandomValueEncoder(object):\n",
    "    def __init__(self, dim=1):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, df):\n",
    "        return torch.rand(len(df), self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_CFG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_pyg_graph\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     sampling_rate\u001b[39m=\u001b[39mMODEL_CFG[\u001b[39m'\u001b[39m\u001b[39mSAMPLING_RATIO\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m     dataset_cfg\u001b[39m=\u001b[39mDATASET_CFG,\n\u001b[1;32m      4\u001b[0m ):\n\u001b[1;32m      5\u001b[0m     data \u001b[39m=\u001b[39m HeteroData()\n\u001b[1;32m      6\u001b[0m     mappings \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_CFG' is not defined"
     ]
    }
   ],
   "source": [
    "def create_pyg_graph(\n",
    "    sampling_rate=MODEL_CFG['SAMPLING_RATIO'],\n",
    "    dataset_cfg=DATASET_CFG,\n",
    "):\n",
    "    data = HeteroData()\n",
    "    mappings = {}\n",
    "    dir_name = f\"{DIR_CFG['DATASETS_DIR']}{sampling_rate}\"\n",
    "    node_file_paths = list(\n",
    "        map(\n",
    "            (lambda cfg: cfg['FILE_NAME']),\n",
    "            dataset_cfg['NODE_TYPES']\n",
    "        )\n",
    "    )\n",
    "    rel_file_paths = list(\n",
    "        map(\n",
    "            (lambda cfg: cfg['FILE_NAME']),\n",
    "            dataset_cfg['REL_TYPES']\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if 'taxon_nodes.csv' in node_file_paths:\n",
    "        taxon_x, taxon_mapping = load_node_tensor(\n",
    "            filename=f'{dir_name}/taxon_nodes.csv',\n",
    "            index_col='appId',\n",
    "            encoders={\n",
    "                # 'rankEncoded': IdentityEncoder(\n",
    "                #     dtype=torch.long, is_tensor=True),\n",
    "                'features': RandomValueEncoder(),\n",
    "                # 'FastRP_embedding': ListEncoder(),\n",
    "            }\n",
    "        )\n",
    "        data['taxon'].x = taxon_x\n",
    "        mappings['taxon'] = taxon_mapping\n",
    "\n",
    "    if 'sotu_nodes.csv' in node_file_paths:\n",
    "        sotu_x, sotu_mapping = load_node_tensor(\n",
    "            filename=f'{dir_name}/sotu_nodes.csv',\n",
    "            index_col='appId',\n",
    "            encoders={\n",
    "                # 'centroidEncoded': IdentityEncoder(\n",
    "                #   dtype=torch.long, is_tensor=True),\n",
    "                'features': RandomValueEncoder(),\n",
    "                # 'FastRP_embedding': ListEncoder(),\n",
    "            }\n",
    "        )\n",
    "        data['sotu'].x = sotu_x #torch.arange(0, len(sotu_mapping))\n",
    "        mappings['sotu'] = sotu_mapping\n",
    "\n",
    "    # if 'tissue_nodes.csv' in node_file_paths:\n",
    "    #     tissue_x, tissue_mapping = load_node_tensor(\n",
    "    #         filename=f'{dir_name}/tissue_nodes.csv',\n",
    "    #         index_col='appId',\n",
    "    #         encoders={\n",
    "    #             # 'centroidEncoded': IdentityEncoder(\n",
    "    #             #   dtype=torch.long, is_tensor=True),\n",
    "    #             'features': ListEncoder(),\n",
    "    #             # 'FastRP_embedding': ListEncoder(),\n",
    "    #         }\n",
    "    #     )\n",
    "    #     data['tissue'].x = tissue_x # torch.arange(0, len(tissue_mapping))\n",
    "    #     mappings['tissue'] = tissue_mapping\n",
    "\n",
    "    if 'sotu_has_host_stat_edges.csv' in rel_file_paths:\n",
    "        edge_index, edge_label = load_edge_tensor(\n",
    "            filename=f'{dir_name}/sotu_has_host_stat_edges.csv',\n",
    "            src_index_col='sourceAppId',\n",
    "            src_mapping=sotu_mapping,\n",
    "            dst_index_col='targetAppId',\n",
    "            dst_mapping=taxon_mapping,\n",
    "            # encoders={\n",
    "            #     'weight': IdentityEncoder(dtype=torch.float, is_tensor=True),\n",
    "            #     'weight': BinaryEncoder(dtype=torch.long),\n",
    "            # },\n",
    "        )\n",
    "        # edge_label = torch.div(edge_label, 100)\n",
    "        data['sotu', 'has_host', 'taxon'].edge_index = edge_index\n",
    "        data['sotu', 'has_host', 'taxon'].edge_label = edge_label\n",
    "\n",
    "    # if 'taxon_has_parent_edges.csv' in rel_file_paths:\n",
    "    #     edge_index, edge_label = load_edge_tensor(\n",
    "    #         filename=f'{dir_name}/taxon_has_parent_edges.csv',\n",
    "    #         src_index_col='sourceAppId',\n",
    "    #         src_mapping=taxon_mapping,\n",
    "    #         dst_index_col='targetAppId',\n",
    "    #         dst_mapping=taxon_mapping,\n",
    "    #         encoders={\n",
    "    #             'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "    #         },\n",
    "    #     )\n",
    "    #     data['taxon', 'has_parent', 'taxon'].edge_index = edge_index\n",
    "    #     data['taxon', 'has_parent', 'taxon'].edge_label = edge_label\n",
    "\n",
    "    # if 'tissue_has_parent_edges.csv' in rel_file_paths:\n",
    "    #     edge_index, edge_label = load_edge_tensor(\n",
    "    #         filename=f'{dir_name}/tissue_has_parent_edges.csv',\n",
    "    #         src_index_col='sourceAppId',\n",
    "    #         src_mapping=tissue_mapping,\n",
    "    #         dst_index_col='targetAppId',\n",
    "    #         dst_mapping=tissue_mapping,\n",
    "    #         encoders={\n",
    "    #             'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "    #         },\n",
    "    #     )\n",
    "    #     data['tissue', 'has_parent', 'tissue'].edge_index = edge_index\n",
    "    #     data['tissue', 'has_parent', 'tissue'].edge_label = edge_label\n",
    "\n",
    "    # if 'sotu_sequence_alignment_edges.csv' in rel_file_paths:\n",
    "    #     edge_index, edge_label = load_edge_tensor(\n",
    "    #         filename=f'{dir_name}/sotu_sequence_alignment_edges.csv',\n",
    "    #         src_index_col='sourceAppId',\n",
    "    #         src_mapping=sotu_mapping,\n",
    "    #         dst_index_col='targetAppId',\n",
    "    #         dst_mapping=sotu_mapping,\n",
    "    #         encoders={\n",
    "    #             'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "    #         },\n",
    "    #     )\n",
    "    #     data['sotu', 'sequence_alignment', 'sotu'].edge_index = edge_index\n",
    "    #     data['sotu', 'sequence_alignment', 'sotu'].edge_label = edge_label\n",
    "\n",
    "    # if 'sotu_has_inferred_taxon_edges.csv' in rel_file_paths:\n",
    "    #     edge_index, edge_label = load_edge_tensor(\n",
    "    #         filename=f'{dir_name}/sotu_has_inferred_taxon_edges.csv',\n",
    "    #         src_index_col='sourceAppId',\n",
    "    #         src_mapping=sotu_mapping,\n",
    "    #         dst_index_col='targetAppId',\n",
    "    #         dst_mapping=taxon_mapping,\n",
    "    #         encoders={\n",
    "    #             'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "    #         },\n",
    "    #     )\n",
    "    #     data['sotu', 'has_inferred_taxon', 'taxon'].edge_index = edge_index\n",
    "    #     data['sotu', 'has_inferred_taxon', 'taxon'].edge_label = edge_label\n",
    "\n",
    "    node_types, edge_types = data.metadata()\n",
    "    data = T.ToUndirected()(data)\n",
    "    # if not ('taxon', 'rev_has_host', 'sotu') in edge_types:\n",
    "    #     data = T.ToUndirected()(data)\n",
    "        # Remove \"reverse\" label. (redundant if using link loader)\n",
    "        # del data['taxon', 'rev_has_host', 'sotu'].edge_label\n",
    "    return data, mappings\n",
    "\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "    num_test = (1 - MODEL_CFG['TRAIN_FRACTION']) * MODEL_CFG['TEST_FRACTION']\n",
    "    num_val = 1 - MODEL_CFG['TRAIN_FRACTION'] - num_test\n",
    "    # labels = data[('sotu', 'has_host', 'taxon')]['edge_label']\n",
    "    # print(labels)\n",
    "    # print(torch.min(labels))\n",
    "    # print(torch.max(labels))\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        # Link-level split train (80%), validate (10%), and test edges (10%)\n",
    "        # num_val=num_val,\n",
    "        num_val=0.1,\n",
    "        # num_test=num_test,\n",
    "        num_test=0.1,\n",
    "        # Of training edges, use 70% for message passing (edge_index)\n",
    "        # and 30% for supervision (edge_label_index)\n",
    "        disjoint_train_ratio=0.3,\n",
    "        # Generate fixed negative edges for evaluation with a ratio of 2-1.\n",
    "        # Negative edges during training will be generated on-the-fly.\n",
    "        neg_sampling_ratio=2.0,\n",
    "        add_negative_train_samples=False,\n",
    "        # is_undirected=True,\n",
    "        edge_types=('sotu', 'has_host', 'taxon'),\n",
    "        rev_edge_types=('taxon', 'rev_has_host', 'sotu'),\n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_loader(data_obj, batch_size=128):\n",
    "    # Define mini-batch loaders\n",
    "    edge_label_index = data_obj[(\n",
    "        'sotu', 'has_host', 'taxon')].edge_label_index\n",
    "\n",
    "    edge_label = data_obj[(\n",
    "        'sotu', 'has_host', 'taxon')].edge_label\n",
    "\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=data_obj,\n",
    "        # In the first hop, we sample at most 20 neighbors.\n",
    "        # In the second hop, we sample at most 10 neighbors.\n",
    "        num_neighbors=[20, 10],\n",
    "        neg_sampling_ratio=3.0, #MODEL_CFG['NEGATIVE_SAMPLING_RATIO']\n",
    "        # neg_sampling='binary',\n",
    "        # let 'binary' setting handle this\n",
    "        # edge_label=train_data[('sotu', 'has_host', 'taxon')].edge_label,\n",
    "        edge_label_index=(('sotu', 'has_host', 'taxon'),\n",
    "                          edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        # num_workers=4,\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def get_val_loader(val_data, batch_size=128*3):\n",
    "    # Define the validation seed edges:\n",
    "    edge_label_index = val_data['sotu', 'has_host', 'taxon'].edge_label_index\n",
    "    edge_label = val_data['sotu', 'has_host', 'taxon'].edge_label\n",
    "\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data=val_data,\n",
    "        num_neighbors=[20, 10],\n",
    "        edge_label_index=(('sotu', 'has_host', 'taxon'),\n",
    "                          edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        # num_workers=4,\n",
    "    )\n",
    "    return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, mappings = create_pyg_graph()\n",
    "train_data, val_data, test_data = split_data(data)\n",
    "\n",
    "train_loader = get_train_loader(train_data)\n",
    "val_loader = get_val_loader(val_data)\n",
    "test_loader = get_val_loader(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412424\n",
      "589177\n",
      "589177\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "[0, 176753]\n",
      "[147294, 73647]\n",
      "----\n",
      "HeteroData(\n",
      "  taxon={\n",
      "    x=[738, 1],\n",
      "    n_id=[738],\n",
      "    num_sampled_nodes=[3],\n",
      "  },\n",
      "  sotu={\n",
      "    x=[2842, 1],\n",
      "    n_id=[2842],\n",
      "    num_sampled_nodes=[3],\n",
      "  },\n",
      "  (sotu, has_host, taxon)={\n",
      "    edge_index=[2, 2510],\n",
      "    edge_label=[512],\n",
      "    edge_label_index=[2, 512],\n",
      "    e_id=[2510],\n",
      "    num_sampled_edges=[2],\n",
      "    input_id=[128],\n",
      "  },\n",
      "  (taxon, rev_has_host, sotu)={\n",
      "    edge_index=[2, 7597],\n",
      "    e_id=[7597],\n",
      "    num_sampled_edges=[2],\n",
      "  }\n",
      ")\n",
      "512\n",
      "tensor(1.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"sotu\", \"has_host\", \"taxon\"].num_edges)\n",
    "print(val_data[\"sotu\", \"has_host\", \"taxon\"].num_edges)\n",
    "print(val_data[\"sotu\", \"has_host\", \"taxon\"].num_edges)\n",
    "\n",
    "print(train_data[\"sotu\", \"has_host\", \"taxon\"].edge_label.min())\n",
    "print(train_data[\"sotu\", \"has_host\", \"taxon\"].edge_label.max())\n",
    "\n",
    "print(train_data[\"sotu\", \"has_host\", \"taxon\"].edge_label.long().bincount().tolist())\n",
    "print(val_data[\"sotu\", \"has_host\", \"taxon\"].edge_label.long().bincount().tolist())\n",
    "\n",
    "\n",
    "print('----')\n",
    "sampled_data = next(iter(train_loader))\n",
    "print(sampled_data)\n",
    "print(sampled_data[\"sotu\", \"has_host\", \"taxon\"].edge_label_index.size(1))\n",
    "print(sampled_data[\"sotu\", \"has_host\", \"taxon\"].edge_label.max())\n",
    "print(sampled_data[\"sotu\", \"has_host\", \"taxon\"].edge_label.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3015049\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(data.num_nodes)\n",
    "# for batch in train_loader:\n",
    "#     print(batch.validate())\n",
    "#     # print(batch.edge_index_dict[('taxon', 'has_parent', 'taxon')].max())\n",
    "#     print(batch.edge_index_dict[('sotu', 'sequence_alignment', 'sotu')].max())\n",
    "#     print(batch.edge_index_dict[('taxon', 'has_parent', 'taxon')].shape )\n",
    "# #     print(edge_index.max())\n",
    "\n",
    "# print(data.num_node_features)\n",
    "print(data.validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z = torch.cat([z_dict['sotu'][row], z_dict['taxon'][col]], dim=-1)\n",
    "\n",
    "        z = self.lin1(z).relu()\n",
    "        z = self.lin2(z)\n",
    "        return z.view(-1)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, data):\n",
    "        super().__init__()\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "\n",
    "def get_model(data):\n",
    "    model = Model(\n",
    "        hidden_channels=64,\n",
    "        data=data,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    total_neg = 0\n",
    "    total = 0\n",
    "    for batch in train_loader:\n",
    "        labels = batch['sotu', 'has_host', 'taxon'].edge_label\n",
    "        total_neg += torch.count_nonzero(labels).item()\n",
    "        total += labels.numel()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(\n",
    "            batch.x_dict,\n",
    "            batch.edge_index_dict,\n",
    "            batch['sotu', 'has_host', 'taxon'].edge_label_index\n",
    "        )\n",
    "\n",
    "        target = batch['sotu', 'has_host', 'taxon'].edge_label.float()\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        total_examples += pred.numel()\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    preds, targets = [], []\n",
    "    total_examples = total_correct = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        pred = model(\n",
    "            batch.x_dict,\n",
    "            batch.edge_index_dict,\n",
    "            batch['sotu', 'has_host', 'taxon'].edge_label_index\n",
    "        )\n",
    "        # pred = pred.sigmoid().view(-1).cpu()\n",
    "        pred = pred.clamp(min=0, max=1)\n",
    "        pred = (pred>0.5).float()\n",
    "\n",
    "        target = batch['sotu', 'has_host', 'taxon'].edge_label.float() #.cpu()\n",
    "        preds.append(pred)\n",
    "        targets.append(target)\n",
    "\n",
    "    pred = torch.cat(preds, dim=0).numpy()\n",
    "    target = torch.cat(targets, dim=0).numpy()\n",
    "\n",
    "    accuracy = accuracy_score(target, pred)\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # auc_roc = roc_auc_score(target, pred)\n",
    "    # print(f\"Validation AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "    auc_pr = average_precision_score(target, pred)\n",
    "    # print(f\"Validation AUC-PR: {auc_pr:.4f}\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def update_stats(training_stats, epoch_stats):\n",
    "    if training_stats is None:\n",
    "        training_stats = {}\n",
    "        for key in epoch_stats.keys():\n",
    "            training_stats[key] = []\n",
    "    for key, val in epoch_stats.items():\n",
    "        training_stats[key].append(val)\n",
    "    return training_stats\n",
    "\n",
    "\n",
    "\n",
    "def train_and_eval_loop(model, train_loader, val_loader, test_loader):\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    training_stats = None\n",
    "\n",
    "    for epoch in range(1, MODEL_CFG['MAX_EPOCHS']):\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        train_acc = test(model, test_loader, device)\n",
    "        val_acc = test(model, val_loader, device)\n",
    "        epoch_stats = {'train_acc': train_acc, 'val_acc': val_acc,\n",
    "                       'train_loss': train_loss, 'epoch': epoch}\n",
    "        training_stats = update_stats(training_stats, epoch_stats)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch:03d}\")\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "            print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "            print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        if epoch > MODEL_CFG['MIN_EPOCHS'] \\\n",
    "                and early_stopper.early_stop(val_acc):\n",
    "            break\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010\n",
      "Train loss: 0.0000\n",
      "Train accuracy: 0.9999\n",
      "Validation accuracy: 0.9998\n",
      "Epoch: 020\n",
      "Train loss: 0.0000\n",
      "Train accuracy: 0.9999\n",
      "Validation accuracy: 0.9998\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m get_model(data)\n\u001b[0;32m----> 2\u001b[0m stats \u001b[39m=\u001b[39m train_and_eval_loop(\n\u001b[1;32m      3\u001b[0m     model, train_loader, val_loader, test_loader)\n",
      "Cell \u001b[0;32mIn[17], line 106\u001b[0m, in \u001b[0;36mtrain_and_eval_loop\u001b[0;34m(model, train_loader, val_loader, test_loader)\u001b[0m\n\u001b[1;32m    103\u001b[0m training_stats \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, MODEL_CFG[\u001b[39m'\u001b[39m\u001b[39mMAX_EPOCHS\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m--> 106\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer, device)\n\u001b[1;32m    107\u001b[0m     train_acc \u001b[39m=\u001b[39m test(model, test_loader, device)\n\u001b[1;32m    108\u001b[0m     val_acc \u001b[39m=\u001b[39m test(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[17], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m target \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39msotu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhas_host\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtaxon\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39medge_label\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     39\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(pred, target)\n\u001b[0;32m---> 40\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     41\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(loss)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnalab/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rnalab/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = get_model(data)\n",
    "stats = train_and_eval_loop(\n",
    "    model, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_acc': [0.9007774140752864, 0.935556464811784, 0.25572831423895254, 0.9392389525368249, 0.25736497545008186, 0.938011456628478, 0.9404664484451718, 0.2569558101472995, 0.2561374795417349, 0.9376022913256956, 0.2569558101472995, 0.9400572831423896, 0.25736497545008186, 0.9384206219312602, 0.9449672667757774, 0.25777414075286414, 0.9433306055646481, 0.9373977086743044, 0.2571603927986907, 0.9435351882160393, 0.2569558101472995, 0.9437397708674304, 0.2579787234042553, 0.978518821603928, 0.2608428805237316, 0.9478314238952537, 0.948240589198036, 0.8788870703764321, 0.2839607201309329, 0.9463993453355155, 0.9445581014729951, 0.9652209492635024, 0.26268412438625205, 0.26063829787234044, 0.9453764320785597, 0.2624795417348609, 0.983633387888707, 0.25941080196399346, 0.9449672667757774, 0.2590016366612111, 0.993657937806874, 0.2600245499181669, 0.9883387888707038, 0.9838379705400983, 0.2590016366612111, 0.9429214402618658, 0.2579787234042553, 0.9468085106382979, 0.2608428805237316, 0.955810147299509, 0.26759410801963995, 0.9451718494271686, 0.25879705400981995, 0.26063829787234044, 0.9484451718494271, 0.25961538461538464, 0.2673895253682488, 0.9478314238952537, 0.26984451718494273, 0.9504909983633388, 0.26779869067103107, 0.9871112929623568, 0.2602291325695581, 0.9523322422258592, 0.263911620294599, 0.9513093289689034, 0.9592880523731587, 0.26166121112929625, 0.9684942716857611, 0.26104746317512273, 0.9867021276595744, 0.265139116202946, 0.9846563011456628, 0.8960720130932897, 0.9797463175122749, 0.9842471358428805, 0.9895662847790507, 0.2632978723404255, 0.9599018003273322, 0.9899754500818331, 0.26207037643207853, 0.9537643207855974, 0.26104746317512273, 0.9463993453355155, 0.9468085106382979, 0.2581833060556465, 0.9490589198036007, 0.9441489361702128, 0.2608428805237316, 0.9926350245499181, 0.9889525368248773, 0.977291325695581, 0.9940671031096563, 0.9486497545008183, 0.948240589198036, 0.2618657937806874, 0.9484451718494271, 0.9455810147299509, 0.25777414075286414], 'val_acc': [0.9104862475442044, 0.9413064833005894, 0.2549115913555992, 0.9480599214145383, 0.25589390962671904, 0.9459724950884086, 0.9459724950884086, 0.25601669941060906, 0.256139489194499, 0.9478143418467584, 0.25663064833005894, 0.956041257367387, 0.25663064833005894, 0.950024557956778, 0.9533398821218074, 0.2567534381139489, 0.9524803536345776, 0.9516208251473477, 0.2568762278978389, 0.9863703339882122, 0.25736738703339884, 0.950024557956778, 0.25785854616895876, 0.9824410609037328, 0.26031434184675833, 0.993614931237721, 0.956532416502947, 0.8824901768172888, 0.29113457760314343, 0.950515717092338, 0.9516208251473477, 0.9659872298624754, 0.26154223968565815, 0.26043713163064836, 0.9602161100196464, 0.26154223968565815, 0.987721021611002, 0.2599459724950884, 0.993983300589391, 0.2579813359528487, 0.9502701375245579, 0.2600687622789784, 0.9526031434184676, 0.9609528487229863, 0.2595776031434185, 0.9546905697445972, 0.25920923379174854, 0.9573919449901768, 0.2630157170923379, 0.9889489194499018, 0.2723477406679764, 0.9508840864440079, 0.2574901768172888, 0.25920923379174854, 0.9517436149312377, 0.25920923379174854, 0.26964636542239684, 0.9963163064833006, 0.2730844793713163, 0.9922642436149313, 0.2717337917485265, 0.9713899803536346, 0.25945481335952847, 0.956041257367387, 0.26215618860510803, 0.9652504911591355, 0.9916502946954814, 0.2605599214145383, 0.9609528487229863, 0.2599459724950884, 0.9598477406679764, 0.2663310412573674, 0.9882121807465619, 0.8953831041257367, 0.981827111984283, 0.9888261296660118, 0.9928781925343811, 0.26129666011787817, 0.9548133595284872, 0.9603388998035364, 0.2619106090373281, 0.9932465618860511, 0.2599459724950884, 0.9516208251473477, 0.9518664047151277, 0.25785854616895876, 0.968320235756385, 0.9557956777996071, 0.2600687622789784, 0.9569007858546169, 0.9921414538310412, 0.962303536345776, 0.993492141453831, 0.9534626719056974, 0.9532170923379175, 0.26043713163064836, 0.9529715127701375, 0.9518664047151277, 0.25712180746561886], 'train_loss': [0.09636253537991238, 0.13068280633654805, 0.05594469170328443, 0.03821892043767701, 0.03637944793545088, 0.05173693331485879, 0.08538170141791361, 0.05963732724103912, 0.07385842655762517, 0.023938499013062584, 0.019036737674582024, 0.023100583721104698, 0.03176216720168992, 0.017756331376670424, 0.025830483475605518, 0.025479294079220238, 0.01660988237190559, 0.007496820436561947, 0.015869774716965695, 0.011497096263737999, 0.01005654327217764, 0.015163280947801524, 0.021162343688565267, 0.03259587404965964, 0.005735770623929762, 0.010854104146629621, 0.004560130512655854, 0.007821639704040926, 0.004010488768451148, 0.014929645653637262, 0.007277049233987562, 0.009425197022246066, 0.003613900240821651, 0.006970918705342445, 0.008284966557388805, 0.013778816073303722, 0.009624191957682908, 0.018046764227060173, 0.019388518044679334, 0.018440422957150322, 0.01602429922200654, 0.020097533926831133, 0.017519439691022258, 0.017852262272187027, 0.020664943415677685, 0.014304570792739793, 0.01389906156667906, 0.0184773627162565, 0.00815194055023443, 0.009021241321501287, 0.011362855055913598, 0.017463466271441033, 0.019398794919496278, 0.033709470652129175, 0.016129471081173363, 0.017380127540001504, 0.022121407591574712, 0.02530376610701291, 0.029978345146741102, 0.00548791107393911, 0.011245676421494804, 0.003961974197050554, 0.00930064282518752, 0.012049409644693478, 0.017129562490310293, 0.005011578799269593, 0.017784166258017483, 0.010315738215969355, 0.009923145735907671, 0.010215902484575387, 0.012484739435479996, 0.005703148884781058, 0.006622386788385394, 0.007379158136302242, 0.0019401492102836821, 0.0025598290216317936, 0.004115918459166481, 0.006366444297390952, 0.005729536231723042, 0.014746989048834518, 0.008962895593159327, 0.008063245131809466, 0.008373817060075688, 0.011584463887489758, 0.019166053219434674, 0.011659703648812253, 0.01683268589981254, 0.01453715065302123, 0.009934854000172716, 0.011684791741316525, 0.0159221733455377, 0.010950536267456954, 0.011714524606245825, 0.020269775546709175, 0.017135913652991313, 0.02361337277190775, 0.035821451005295726, 0.07688524524045653, 0.038066443366817096], 'epoch': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]}\n"
     ]
    }
   ],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 62423.2227, Train: 1.0000, Val: 0.7471, Test: 0.7601\n",
      "Epoch: 002, Loss: 3526901.5000, Train: 1.0000, Val: 0.8859, Test: 0.8855\n",
      "Epoch: 003, Loss: 1013483.3125, Train: 1.0000, Val: 0.4918, Test: 0.4982\n",
      "Epoch: 004, Loss: 895018.1250, Train: 1.0000, Val: 0.8454, Test: 0.8556\n",
      "Epoch: 005, Loss: 976447.0625, Train: 1.0000, Val: 0.7759, Test: 0.7992\n",
      "Epoch: 006, Loss: 787798.8750, Train: 1.0000, Val: 0.7489, Test: 0.7837\n",
      "Epoch: 007, Loss: 451444.2188, Train: 1.0000, Val: 0.7469, Test: 0.7364\n",
      "Epoch: 008, Loss: 165784.0625, Train: 1.0000, Val: 0.4272, Test: 0.4235\n",
      "Epoch: 009, Loss: 258998.6406, Train: 1.0000, Val: 0.7814, Test: 0.7692\n",
      "Epoch: 010, Loss: 137901.4375, Train: 1.0000, Val: 0.8090, Test: 0.7883\n",
      "Epoch: 011, Loss: 175027.7344, Train: 1.0000, Val: 0.8116, Test: 0.7865\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/rnalab/lib/python3.11/site-packages/torch_geometric/data/storage.py:85\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rnalab/lib/python3.11/site-packages/torch_geometric/data/storage.py:111\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mapping[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'x'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     48\u001b[0m train_rmse \u001b[39m=\u001b[39m test(train_data)\n\u001b[0;32m---> 49\u001b[0m val_rmse \u001b[39m=\u001b[39m test(val_data)\n\u001b[1;32m     50\u001b[0m test_rmse \u001b[39m=\u001b[39m test(test_data)\n\u001b[1;32m     51\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train: \u001b[39m\u001b[39m{\u001b[39;00mtrain_rmse\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     52\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVal: \u001b[39m\u001b[39m{\u001b[39;00mval_rmse\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test: \u001b[39m\u001b[39m{\u001b[39;00mtest_rmse\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnalab/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m(data):\n\u001b[1;32m     33\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     34\u001b[0m     pred \u001b[39m=\u001b[39m model(\n\u001b[0;32m---> 35\u001b[0m         data\u001b[39m.\u001b[39mx_dict,\n\u001b[1;32m     36\u001b[0m         data\u001b[39m.\u001b[39medge_index_dict,\n\u001b[1;32m     37\u001b[0m         data[\u001b[39m'\u001b[39m\u001b[39msotu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhas_host\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtaxon\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39medge_label_index\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m     pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m     40\u001b[0m     target \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39msotu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhas_host\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtaxon\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39medge_label\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/miniconda3/envs/rnalab/lib/python3.11/site-packages/torch_geometric/data/hetero_data.py:155\u001b[0m, in \u001b[0;36mHeteroData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_global_store, key)\n\u001b[1;32m    154\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mbool\u001b[39m(re\u001b[39m.\u001b[39msearch(\u001b[39m'\u001b[39m\u001b[39m_dict$\u001b[39m\u001b[39m'\u001b[39m, key)):\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollect(key[:\u001b[39m-\u001b[39m\u001b[39m5\u001b[39m])\n\u001b[1;32m    156\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has no \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mattribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnalab/lib/python3.11/site-packages/torch_geometric/data/hetero_data.py:553\u001b[0m, in \u001b[0;36mHeteroData.collect\u001b[0;34m(self, key, allow_empty)\u001b[0m\n\u001b[1;32m    550\u001b[0m mapping \u001b[39m=\u001b[39m {}\n\u001b[1;32m    551\u001b[0m \u001b[39mfor\u001b[39;00m subtype, store \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_node_store_dict\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m    552\u001b[0m                             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edge_store_dict\u001b[39m.\u001b[39mitems()):\n\u001b[0;32m--> 553\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(store, key):\n\u001b[1;32m    554\u001b[0m         mapping[subtype] \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(store, key)\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_empty \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(mapping) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rnalab/lib/python3.11/site-packages/torch_geometric/data/storage.py:87\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[1;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = get_model(data)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# weight = torch.bincount(train_data['sotu', 'has_host', 'taxon'].edge_label)\n",
    "# weight = weight.max() / weight\n",
    "\n",
    "weight = None\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight=None):\n",
    "    weight = 1. if weight is None else weight[target].to(pred.dtype)\n",
    "    return (weight * (pred - target.to(pred.dtype)).pow(2)).mean()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(\n",
    "        train_data.x_dict,\n",
    "        train_data.edge_index_dict,\n",
    "        train_data['sotu', 'has_host', 'taxon'].edge_label_index\n",
    "    )\n",
    "    target = train_data['sotu', 'has_host', 'taxon'].edge_label\n",
    "    # loss = weighted_mse_loss(pred, target, weight)\n",
    "    # loss = F.binary_cross_entropy_with_logits(pred, target)\n",
    "    loss = F.cross_entropy(pred, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    pred = model(\n",
    "        data.x_dict,\n",
    "        data.edge_index_dict,\n",
    "        data['sotu', 'has_host', 'taxon'].edge_label_index\n",
    "    )\n",
    "    pred = pred.clamp(min=0, max=5)\n",
    "    target = data['sotu', 'has_host', 'taxon'].edge_label.float()\n",
    "    # rmse = F.mse_loss(pred, target).sqrt()\n",
    "    auc_pr = average_precision_score(target, pred)\n",
    "    return float(auc_pr)\n",
    "\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    loss = train()\n",
    "    train_rmse = test(train_data)\n",
    "    val_rmse = test(val_data)\n",
    "    test_rmse = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "          f'Val: {val_rmse:.4f}, Test: {test_rmse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnalab-kernel",
   "language": "python",
   "name": "rnalab-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "448a416c49845ffbddc886562179757704d684dae284cc536cfa1eca10a7a7d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
