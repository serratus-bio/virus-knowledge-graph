{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI STAT Host associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan for adding missing runs from toxo dataset\n",
    "\n",
    "1. Get list of runs in dataset that are missing from sra_stat\n",
    "1. Fetch missing runs from bigquery STAT\n",
    "1. Fetch sra_stat_group to dataframe\n",
    "1. Compute additional values for new rows, (groupby total kmers, percent, label)\n",
    "1. Write missing values to psql\n",
    "1. Write all STAT data from toxo runs to csv\n",
    "1. Import, visualize and compare data in seperate graphistry notebook\n",
    "\n",
    "#### Plan for full sra_stat ETL\n",
    "\n",
    "1. Set up notebook in instance with more memory\n",
    "1. Download all stat rows, excluding existing runs using last updated date \n",
    "    - last inserted in sra_stat: (row_id: 85551530, run: \"SRR9999999\")\n",
    "1. Use dask dataframe to compute additional values for new rows (groupby total kmers, percent, label)\n",
    "1. Write new STAT tax values to psql\n",
    "1. Write new edge values to neo4j using containerized ETL job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from queries import serratus_queries, graph_queries\n",
    "from datasources import psql\n",
    "\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_path = '../../graph_learning/notebooks/tgav_data/'\n",
    "toxo_data_path = base_data_path + 'toxo/'\n",
    "\n",
    "def fetch_one(query, params={}):\n",
    "    conn = psql.get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    resp = cursor.execute(query, params)\n",
    "    resp = cursor.fetchone()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return resp\n",
    "\n",
    "def fetch_count(query, params={}):\n",
    "    resp = fetch_one(query, params)\n",
    "    return int(resp[0])\n",
    "\n",
    "def fetch_all(query, params={}):\n",
    "    conn = psql.get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    resp = cursor.execute(query, params)\n",
    "    resp = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return resp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect missing STAT runs in Toxo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/wsxpm_412lgcfm03dqd31qmh0000gn/T/ipykernel_25141/232920902.py:6: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(toxo_data_path + 'toxo_SraRunInfo.csv')\n",
      "/var/folders/vl/wsxpm_412lgcfm03dqd31qmh0000gn/T/ipykernel_25141/232920902.py:15: DtypeWarning: Columns (25,26,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sra_union_metadata = pd.read_csv(toxo_data_path + 'tg_set_all_metadata_additional.csv')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_toxo_dfs():\n",
    "    # Get RunID, TaxID pairs from original datasets\n",
    "    df1 = pd.read_csv(toxo_data_path + 'toxo_SraRunInfo.csv')\n",
    "    df2 = pd.read_csv(toxo_data_path + 'txid5810_SraRunInfo.csv')\n",
    "    df3 = pd.read_csv(toxo_data_path + 'txid5810_statbigquery.csv')\n",
    "    df3 = df3.rename(columns={'tax_id': 'TaxID', 'acc': 'Run'})\n",
    "    sra_union = pd.concat([df1[['Run', 'TaxID']], df2[['Run', 'TaxID']], df3[['Run', 'TaxID']]], axis=0)\n",
    "    sra_union = sra_union.drop_duplicates(subset=['Run'])\n",
    "    sra_union = sra_union.astype({\"Run\": str, \"TaxID\": int})\n",
    "\n",
    "    # Get additional biosample metadata (missing TaxId)\n",
    "    sra_union_metadata = pd.read_csv(toxo_data_path + 'tg_set_all_metadata_additional.csv')\n",
    "    sra_union_metadata = sra_union_metadata.rename(columns={'acc': 'Run'})\n",
    "    sra_union = sra_union_metadata.merge(\n",
    "        sra_union,\n",
    "        left_on='Run',\n",
    "        right_on='Run',\n",
    "        how='left',\n",
    "    ).drop_duplicates(subset=['Run'])\n",
    "\n",
    "    # Get intersection of original datasets\n",
    "    sra_intersection = df1[['Run', 'TaxID']].merge(\n",
    "        df2[['Run', 'TaxID']],\n",
    "        left_on='Run',\n",
    "        right_on='Run',\n",
    "        how='left',\n",
    "    ).dropna()\n",
    "    sra_intersection = sra_intersection.merge(\n",
    "        df3[['Run', 'TaxID']],\n",
    "        left_on='Run',\n",
    "        right_on='Run',\n",
    "        how='left',\n",
    "    ).dropna()\n",
    "    sra_intersection = sra_intersection.astype({\"Run\": str, \"TaxID\": int, \"TaxID_x\": int, \"TaxID_y\": int })\n",
    "\n",
    "    return sra_union, sra_intersection\n",
    "\n",
    "\n",
    "sra_union, sra_intersection = get_toxo_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29039\n",
      "0.8942506081975795\n"
     ]
    }
   ],
   "source": [
    "unique_runs = list(sra_union.Run.unique())\n",
    "\n",
    "def get_matching_sra_counts(runs):\n",
    "    query = \"\"\"\n",
    "        SELECT COUNT(distinct run) \n",
    "        FROM public.sra_stat \n",
    "        WHERE run IN %(runs)s;\n",
    "    \"\"\" \n",
    "    params = {\n",
    "        'runs': tuple(runs),\n",
    "    }\n",
    "    return fetch_count(query, params)\n",
    "\n",
    "counts = get_matching_sra_counts(unique_runs)\n",
    "print(counts)\n",
    "print(counts/len(unique_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728819\n",
      "['DRR001705', 'DRR001705', 'DRR001705', 'DRR001705', 'DRR001705', 'DRR001705', 'DRR001706', 'DRR001706', 'DRR001706', 'DRR001706']\n",
      "1565\n"
     ]
    }
   ],
   "source": [
    "def get_matching_sra_runs(runs):\n",
    "    query = \"\"\"\n",
    "        SELECT run \n",
    "        FROM public.sra_stat \n",
    "        WHERE run IN %(runs)s;\n",
    "    \"\"\"  \n",
    "    params = {\n",
    "        'runs': tuple(runs),\n",
    "    }\n",
    "    return fetch_all(query, params)\n",
    "\n",
    "matched_runs = get_matching_sra_runs(unique_runs)\n",
    "matched_runs = [x[0] for x in matched_runs]\n",
    "print(len(matched_runs))\n",
    "print(matched_runs[:10])\n",
    "\n",
    "missing_runs = list(set(unique_runs) - set(matched_runs))\n",
    "print(len(missing_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12483673\n"
     ]
    }
   ],
   "source": [
    "def get_existing_stat_runs():\n",
    "    query = \"\"\"\n",
    "        SELECT distinct run \n",
    "        FROM public.sra_stat \n",
    "    \"\"\"\n",
    "    resp = fetch_all(query)\n",
    "    return resp\n",
    "\n",
    "# existing_stat_runs = get_existing_stat_runs()\n",
    "# print(len(existing_stat_runs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update sra_stat table with missing Toxo runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=Xz054jgYLJylWcx95CCKPKqI4uy3ti&access_type=offline&code_challenge=qzYZuRWYBxNGQJSQe_BlXQ2_j8yEC-jNAmlxzqnDCgE&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [/Users/lukepereira/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\u001b[1;33mWARNING:\u001b[0m \n",
      "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
     ]
    }
   ],
   "source": [
    "# Authenticate GCP\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukepereira/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "project_id = 'rnalab-393418'\n",
    "client = bigquery.Client(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM nih-sra-datastore.sra_tax_analysis_tool.tax_analysis\n",
    "    WHERE rank = 'order'\n",
    "    AND total_count >= 100\n",
    "    AND acc in UNNEST(@missing_runs)\n",
    "    ORDER BY acc\n",
    "\"\"\"\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    query_parameters=[\n",
    "        bigquery.ArrayQueryParameter(\"missing_runs\", \"STRING\", missing_runs),\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_job = client.query(query, job_config=job_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "          run   taxid   rank                name     kmer\n",
      "0  SRR8144089  213115  order  Desulfovibrionales     1796\n",
      "1  SRR8144089  162474  order       Malasseziales      866\n",
      "2  SRR8144089  186826  order     Lactobacillales  3669547\n",
      "3  SRR8144089    4892  order   Saccharomycetales     1684\n",
      "4  SRR8144089   73020  order      Isochrysidales     5637\n"
     ]
    }
   ],
   "source": [
    "stats_data = []\n",
    "for row in query_job:\n",
    "    # Row values can be accessed by field name or index.\n",
    "    stats_data.append({\n",
    "        'run': row[0],\n",
    "        'taxid': row[1],\n",
    "        'rank': row[2],\n",
    "        'name': row[3],\n",
    "        'kmer': row[4],\n",
    "    })\n",
    "\n",
    "stats_data = pd.DataFrame(stats_data)\n",
    "print(len(stats_data))\n",
    "print(stats_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          run   taxid   rank                name     kmer  total_kmers  \\\n",
      "0  SRR8144089  213115  order  Desulfovibrionales     1796      7259892   \n",
      "1  SRR8144089  162474  order       Malasseziales      866      7259892   \n",
      "2  SRR8144089  186826  order     Lactobacillales  3669547      7259892   \n",
      "3  SRR8144089    4892  order   Saccharomycetales     1684      7259892   \n",
      "4  SRR8144089   73020  order      Isochrysidales     5637      7259892   \n",
      "\n",
      "   kmer_perc  \n",
      "0       0.02  \n",
      "1       0.01  \n",
      "2      50.55  \n",
      "3       0.02  \n",
      "4       0.08  \n"
     ]
    }
   ],
   "source": [
    "total_kmer = stats_data.groupby('run').agg({'kmer': 'sum'}).reset_index()\n",
    "\n",
    "assert stats_data.run.nunique() == total_kmer.shape[0]\n",
    "\n",
    "total_kmer = total_kmer.rename(columns={'kmer': 'total_kmers'})\n",
    "\n",
    "stats_data_totals = stats_data.merge(\n",
    "    total_kmer[['run', 'total_kmers']],\n",
    "    left_on='run',\n",
    "    right_on='run',\n",
    ")\n",
    "stats_data_totals['kmer_perc'] = (stats_data_totals['kmer'] / stats_data_totals['total_kmers']) * 100\n",
    "stats_data_totals['kmer_perc'] = stats_data_totals['kmer_perc'].round(2)\n",
    "# Note, existing table contains 15,552,449/85,551,530 (20%) that have kmer_perc == 0.0\n",
    "\n",
    "print(stats_data_totals.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184\n",
      "[(29, 'order', 'Myxococcales', 'Bacteria'), (112, 'order', 'Planctomycetales', 'Bacteria'), (136, 'order', 'Spirochaetales', 'Bacteria'), (356, 'order', 'Hyphomicrobiales', 'Bacteria'), (766, 'order', 'Rickettsiales', 'Bacteria'), (1118, 'order', 'Chroococcales', 'Bacteria'), (1150, 'order', 'Oscillatoriales', 'Bacteria'), (1161, 'order', 'Nostocales', 'Bacteria'), (1189, 'order', 'Stigonemataceae', 'Bacteria'), (1212, 'order', 'NA', 'Unclassified')]\n"
     ]
    }
   ],
   "source": [
    "def get_stats_tax_labels():\n",
    "    query = \"\"\"\n",
    "        SELECT * FROM public.sra_stat_group\n",
    "        ORDER BY taxid ASC\n",
    "    \"\"\"\n",
    "    return fetch_all(query)\n",
    "\n",
    "stats_tax_labels = get_stats_tax_labels()\n",
    "print(len(stats_tax_labels))\n",
    "print(stats_tax_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          run   taxid   rank                name     kmer  total_kmers  \\\n",
      "0  SRR8144089  213115  order  Desulfovibrionales     1796      7259892   \n",
      "1  SRR8144089  162474  order       Malasseziales      866      7259892   \n",
      "2  SRR8144089  186826  order     Lactobacillales  3669547      7259892   \n",
      "3  SRR8144089    4892  order   Saccharomycetales     1684      7259892   \n",
      "4  SRR8144089   73020  order      Isochrysidales     5637      7259892   \n",
      "\n",
      "   kmer_perc  tax_label  \n",
      "0       0.02   Bacteria  \n",
      "1       0.01      Fungi  \n",
      "2      50.55   Bacteria  \n",
      "3       0.02      Fungi  \n",
      "4       0.08  Eukaryota  \n"
     ]
    }
   ],
   "source": [
    "stats_tax_labels = pd.DataFrame(stats_tax_labels, columns=['taxid', 'tax_rank', 'tax_name', 'tax_label'])\n",
    "\n",
    "stats_data_totals_labels = stats_data_totals.merge(\n",
    "    stats_tax_labels[['taxid', 'tax_label']],\n",
    "    left_on='taxid',\n",
    "    right_on='taxid',\n",
    "    how='left',\n",
    ")\n",
    "\n",
    "assert stats_data_totals_labels.shape[0] == stats_data_totals.shape[0]\n",
    "print(stats_data_totals_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data_totals_labels.to_csv('tmp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data_totals_labels = pd.read_csv('tmp.csv')\n",
    "print(len(stats_data_totals_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_write_connection():\n",
    "    return psycopg2.connect(\n",
    "        database=\"summary\",\n",
    "        host=\"serratus-aurora-20210406.cluster-ro-ccz9y6yshbls.us-east-1.rds.amazonaws.com\",\n",
    "        user=os.environ.get('SQL_WRITE_USER'),\n",
    "        password=os.environ.get('SQL_WRITE_PASSWORD'),\n",
    "        port=\"5432\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_row_id():\n",
    "    conn = psql.get_connection()\n",
    "    cursor = conn.cursor()\n",
    "    query = \"\"\"\n",
    "        SELECT max(CAST(row_id as Int)) FROM sra_stat;\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    out = cursor.fetchone()[0]\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return int(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stats_table(df):\n",
    "    cur_row_id = get_max_row_id() + 1\n",
    "    conn = get_write_connection()\n",
    "    cursor = conn.cursor()\n",
    "    errors = []\n",
    "    \n",
    "    for row in df.reset_index().to_dict('rows'):\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "                INSERT into public.sra_stat(row_id, run, taxid, \n",
    "                    rank, name, kmer, \n",
    "                    total_kmers, kmer_perc, tax_label\n",
    "                )\n",
    "                VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT DO NOTHING;\n",
    "            \"\"\" \n",
    "            args = (\n",
    "                cur_row_id, row['run'], row['taxid'],\n",
    "                row['rank'], row['name'], row['kmer'], \n",
    "                row['total_kmers'], row['kmer_perc'], row['tax_label'],\n",
    "            )\n",
    "            cursor.execute(query, args)\n",
    "            if cur_row_id % 1000 == 0:\n",
    "                print(cur_row_id)\n",
    "                conn.commit()\n",
    "            cur_row_id += 1\n",
    "        except Exception as e:\n",
    "            errors.append(row)\n",
    "            print(e)\n",
    "            conn.rollback()\n",
    "            break\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/wsxpm_412lgcfm03dqd31qmh0000gn/T/ipykernel_25141/2124966447.py:7: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  for row in df.reset_index().to_dict('rows'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "errs = update_stats_table(stats_data_totals_labels)\n",
    "print(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30908\n"
     ]
    }
   ],
   "source": [
    "# Fetch all toxo runs from sra_stat\n",
    "def get_toxo_runs():\n",
    "    query = \"\"\"\n",
    "        SELECT DISTINCT ON (run)\n",
    "            run,\n",
    "            taxid,\n",
    "            rank,\n",
    "            name,\n",
    "            kmer,\n",
    "            total_kmers,\n",
    "            kmer_perc,\n",
    "            tax_label\n",
    "        FROM public.sra_stat\n",
    "        WHERE run in %(toxo_runs)s\n",
    "        ORDER BY run, kmer_perc DESC;\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'toxo_runs': tuple(unique_runs),\n",
    "    }\n",
    "    return fetch_all(query, params)\n",
    "\n",
    "toxo_sra_stat = get_toxo_runs()\n",
    "print(len(toxo_sra_stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DRR001705', 9989, 'order', 'Rodentia', 988967, Decimal('994240'), Decimal('99.47'), 'Mammalia'), ('DRR001706', 9989, 'order', 'Rodentia', 1438884, Decimal('1447150'), Decimal('99.43'), 'Mammalia'), ('DRR002461', 75739, 'order', 'Eucoccidiorida', 8963466, Decimal('11776961'), Decimal('76.11'), 'Eukaryota'), ('DRR002462', 75739, 'order', 'Eucoccidiorida', 5861322, Decimal('10788973'), Decimal('54.33'), 'Eukaryota'), ('DRR002463', 75739, 'order', 'Eucoccidiorida', 8563615, Decimal('10225673'), Decimal('83.75'), 'Eukaryota'), ('DRR002464', 75739, 'order', 'Eucoccidiorida', 8824039, Decimal('11567974'), Decimal('76.28'), 'Eukaryota'), ('DRR002465', 75739, 'order', 'Eucoccidiorida', 5740362, Decimal('10530519'), Decimal('54.51'), 'Eukaryota'), ('DRR002466', 75739, 'order', 'Eucoccidiorida', 8423467, Decimal('10036212'), Decimal('83.93'), 'Eukaryota'), ('DRR014676', 9443, 'order', 'Primates', 21622218, Decimal('21654403'), Decimal('99.85'), 'Primates'), ('DRR022972', 9443, 'order', 'Primates', 1176895, Decimal('1178198'), Decimal('99.89'), 'Primates')]\n"
     ]
    }
   ],
   "source": [
    "print(toxo_sra_stat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728819\n",
      "1565\n",
      "['SRR18548922', 'SRR11063004', 'SRR18532781', 'SRR19329101', 'ERR5466811', 'SRR19329048', 'SRR20330572', 'SRR11156255', 'SRR20330436', 'SRR19325907']\n"
     ]
    }
   ],
   "source": [
    "matched_runs_v2 = get_matching_sra_runs(unique_runs)\n",
    "matched_runs_v2 = [x[0] for x in matched_runs_v2]\n",
    "print(len(matched_runs_v2))\n",
    "\n",
    "missing_runs_v2 = list(set(unique_runs) - set(matched_runs_v2))\n",
    "print(len(missing_runs_v2))\n",
    "\n",
    "print(missing_runs_v2[:10])\n",
    "\n",
    "# These runs are missing from STAT bigquery table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv in tgav_data\n",
    "toxo_sra_stat = pd.DataFrame(toxo_sra_stat, columns=['run', 'taxid', 'rank', 'name', 'kmer', 'total_kmers', 'kmer_perc', 'tax_label'])\n",
    "toxo_sra_stat.to_csv(toxo_data_path + 'tg_tax_host_stat.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Neo4j with STAT HAS_HOST edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30908\n"
     ]
    }
   ],
   "source": [
    "toxo_sra_stat = pd.read_csv(toxo_data_path + 'tg_tax_host_stat.csv')\n",
    "toxo_sra_stat['taxid'] = toxo_sra_stat['taxid'].astype(str)\n",
    "toxo_sra_stat_ddf = dd.from_pandas(toxo_sra_stat, chunksize=1000)\n",
    "print(len(toxo_sra_stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30908\n",
      "22942\n",
      "237\n",
      "233\n",
      "{'28883', '206350', '1212', '40677'}\n"
     ]
    }
   ],
   "source": [
    "from datasources.neo4j import get_connection\n",
    "\n",
    "conn = get_connection()\n",
    "\n",
    "def sanity_check_sras(run_ids):\n",
    "    query = '''\n",
    "            MATCH (s:SRA)\n",
    "            WHERE s.runId in $run_ids\n",
    "            RETURN COLLECT(DISTINCT s.runId) as run_ids\n",
    "            '''\n",
    "    return conn.query(\n",
    "            query,\n",
    "            parameters={\n",
    "                'run_ids': run_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "def sanity_check_taxons(tax_ids):\n",
    "    query = '''\n",
    "            MATCH (t:Taxon)\n",
    "            WHERE t.taxId in $tax_ids\n",
    "            RETURN Collect(DISTINCT t.taxId) as tax_ids\n",
    "            '''\n",
    "    return conn.query(\n",
    "            query,\n",
    "            parameters={\n",
    "                'tax_ids': tax_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "out = sanity_check_sras(toxo_sra_stat.run.unique())\n",
    "print(len(toxo_sra_stat.run.unique()))\n",
    "print(len(out[0]['run_ids']))\n",
    "missing_runs = set(toxo_sra_stat.run.unique()) - set(out[0]['run_ids'])\n",
    "\n",
    "taxons = toxo_sra_stat.taxid.unique()\n",
    "taxons = [str(x) for x in taxons]\n",
    "out = sanity_check_taxons(taxons)\n",
    "missing_taxons = set(taxons) - set(out[0]['tax_ids'])\n",
    "print(len(taxons))\n",
    "print(len(out[0]['tax_ids']))\n",
    "print(missing_taxons)\n",
    "\n",
    "# 28883 was merged to 2731619\n",
    "# 206350 was merged to 32003\n",
    "# 1212 was merged to 1213 \n",
    "# 40677 was merged to 6132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         run  taxid   rank            name     kmer  total_kmers  kmer_perc  \\\n",
      "0  DRR001705   9989  order        Rodentia   988967       994240      99.47   \n",
      "1  DRR001706   9989  order        Rodentia  1438884      1447150      99.43   \n",
      "2  DRR002461  75739  order  Eucoccidiorida  8963466     11776961      76.11   \n",
      "3  DRR002462  75739  order  Eucoccidiorida  5861322     10788973      54.33   \n",
      "4  DRR002463  75739  order  Eucoccidiorida  8563615     10225673      83.75   \n",
      "\n",
      "   tax_label  \n",
      "0   Mammalia  \n",
      "1   Mammalia  \n",
      "2  Eukaryota  \n",
      "3  Eukaryota  \n",
      "4  Eukaryota  \n"
     ]
    }
   ],
   "source": [
    "print(toxo_sra_stat_ddf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "30908\n"
     ]
    }
   ],
   "source": [
    "from datasources.neo4j import get_connection\n",
    "\n",
    "conn = get_connection()\n",
    "\n",
    "def debug_missing_spots(run_ids):\n",
    "    query = '''\n",
    "            MATCH (s:SRA)\n",
    "            WHERE s.runId in $run_ids\n",
    "            AND s.spots = 0\n",
    "            RETURN Collect(DISTINCT s.runId) as run_ids\n",
    "            '''\n",
    "    return conn.query(\n",
    "            query,\n",
    "            parameters={\n",
    "                'run_ids': run_ids,\n",
    "            }\n",
    "        )\n",
    "\n",
    "out = debug_missing_spots(toxo_sra_stat.run.unique())\n",
    "print(len(out[0]['run_ids']))\n",
    "print(len(toxo_sra_stat.run.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temp fix in graph to add missing spots\n",
    "# Long-term fix involves updating sra tables to include missing spots\n",
    "missing_spots = {\n",
    "      'ERR3415775': 27250560,\n",
    "      'ERR4352771': 0,\n",
    "      'ERR1994964': 23972947,\n",
    "      'ERR5384467': 8430905,\n",
    "      'ERR1726732': 0,\n",
    "      'ERR3415759': 27860746,\n",
    "      'ERR3274949': 865925712,\n",
    "      'ERR538188': 0,\n",
    "      'ERR2003549': 23972947,\n",
    "      'ERR3415758': 41826118,\n",
    "      'ERR3415760': 31480217,\n",
    "      'ERR1726762': 0,\n",
    "      'ERR1726702': 0,\n",
    "      'ERR1994972': 23231253,\n",
    "      'ERR1726688': 0,\n",
    "      'ERR3415762': 20223464,\n",
    "      'ERR2003534': 27306569,\n",
    "      'ERR3978063': 60335336,\n",
    "      'ERR3806953': 0,\n",
    "      'ERR3415774': 35643841,\n",
    "      'ERR2003542': 36240950,\n",
    "      'ERR3415773': 39138550,\n",
    "      'ERR538183': 0,\n",
    "      'ERR3415761': 24380142,\n",
    "      'ERR2003547': 23729256,\n",
    "      'ERR1726916': 0,\n",
    "      'ERR1726629': 0,\n",
    "      'ERR1726949': 0,\n",
    "      'ERR2003527': 35611500,\n",
    "      'ERR1994960': 36240950,\n",
    "      'ERR1726891': 0,\n",
    "      'ERR1726944': 0,\n",
    "      'ERR1726740': 0,\n",
    "      'ERR1726724': 0,\n",
    "      'ERR3806950': 0,\n",
    "      'ERR4352608': 0,\n",
    "}\n",
    "\n",
    "missing_spots_df = pd.DataFrame(missing_spots.items(), columns=['runId', 'spots'])\n",
    "\n",
    "missing_spots_ddf = dd.from_pandas(missing_spots_df, chunksize=1000)\n",
    "\n",
    "\n",
    "def add_missing_spots_to_sras(rows):\n",
    "    query = '''\n",
    "            UNWIND $rows as row\n",
    "            MATCH (s:SRA)\n",
    "            WHERE s.runId = row.run \n",
    "            SET s += {\n",
    "                spots: toInteger(row.spots),\n",
    "                spotsWithMates: toInteger(row.spots)\n",
    "            }\n",
    "            '''\n",
    "    return graph_queries.batch_insert_data(query, rows)\n",
    "\n",
    "add_missing_spots_to_sras(missing_spots_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_sra_stat_taxon_edges(rows):\n",
    "    query = '''\n",
    "            UNWIND $rows as row\n",
    "            MATCH (s:SRA), (t:Taxon)\n",
    "            WHERE s.runId = row.run AND t.taxId = row.taxid\n",
    "            MERGE (s)-[r:HAS_HOST_STAT]->(t)\n",
    "            SET r += {\n",
    "                percentIdentity: round(toFloat(row.kmer_perc / 100), 4),\n",
    "                percentIdentityFull: CASE WHEN s.spots > 0 \n",
    "                    THEN round(toFloat(row.kmer) / toFloat(s.spots), 4)\n",
    "                    ELSE 0.0 END,\n",
    "                kmer: row.kmer,\n",
    "                totalKmers: row.total_kmers,\n",
    "                totalSpots: s.spots\n",
    "            }\n",
    "            \n",
    "\n",
    "            '''\n",
    "    return graph_queries.batch_insert_data(query, rows)\n",
    "\n",
    "add_sra_stat_taxon_edges(toxo_sra_stat_ddf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update sra_stat table with all missing runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39m    SELECT MAX(updated)\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m    FROM nih-sra-datastore.sra_tax_analysis_tool.tax_analysis\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m    ORDER BY acc\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     10\u001b[0m job_config \u001b[39m=\u001b[39m bigquery\u001b[39m.\u001b[39mQueryJobConfig(\n\u001b[1;32m     11\u001b[0m     query_parameters\u001b[39m=\u001b[39m[\n\u001b[1;32m     12\u001b[0m         bigquery\u001b[39m.\u001b[39mArrayQueryParameter(\u001b[39m\"\u001b[39m\u001b[39mexisting_runs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSTRING\u001b[39m\u001b[39m\"\u001b[39m, existing_stat_runs),\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m query_job \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mquery(query, job_config\u001b[39m=\u001b[39mjob_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/cloud/bigquery/client.py:3408\u001b[0m, in \u001b[0;36mClient.query\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3397\u001b[0m     \u001b[39mreturn\u001b[39;00m _job_helpers\u001b[39m.\u001b[39mquery_jobs_query(\n\u001b[1;32m   3398\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m   3399\u001b[0m         query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3405\u001b[0m         job_retry,\n\u001b[1;32m   3406\u001b[0m     )\n\u001b[1;32m   3407\u001b[0m \u001b[39melif\u001b[39;00m api_method \u001b[39m==\u001b[39m enums\u001b[39m.\u001b[39mQueryApiMethod\u001b[39m.\u001b[39mINSERT:\n\u001b[0;32m-> 3408\u001b[0m     \u001b[39mreturn\u001b[39;00m _job_helpers\u001b[39m.\u001b[39mquery_jobs_insert(\n\u001b[1;32m   3409\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m   3410\u001b[0m         query,\n\u001b[1;32m   3411\u001b[0m         job_config,\n\u001b[1;32m   3412\u001b[0m         job_id,\n\u001b[1;32m   3413\u001b[0m         job_id_prefix,\n\u001b[1;32m   3414\u001b[0m         location,\n\u001b[1;32m   3415\u001b[0m         project,\n\u001b[1;32m   3416\u001b[0m         retry,\n\u001b[1;32m   3417\u001b[0m         timeout,\n\u001b[1;32m   3418\u001b[0m         job_retry,\n\u001b[1;32m   3419\u001b[0m     )\n\u001b[1;32m   3420\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3421\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unexpected value for api_method: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(api_method)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/cloud/bigquery/_job_helpers.py:114\u001b[0m, in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         \u001b[39mreturn\u001b[39;00m query_job\n\u001b[0;32m--> 114\u001b[0m future \u001b[39m=\u001b[39m do_query()\n\u001b[1;32m    115\u001b[0m \u001b[39m# The future might be in a failed state now, but if it's\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m# unrecoverable, we'll find out when we ask for it's result, at which\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m# point, we may retry.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m job_id_given:\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/cloud/bigquery/_job_helpers.py:91\u001b[0m, in \u001b[0;36mquery_jobs_insert.<locals>.do_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m query_job \u001b[39m=\u001b[39m job\u001b[39m.\u001b[39mQueryJob(job_ref, query, client\u001b[39m=\u001b[39mclient, job_config\u001b[39m=\u001b[39mjob_config)\n\u001b[1;32m     90\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     query_job\u001b[39m.\u001b[39m_begin(retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m     92\u001b[0m \u001b[39mexcept\u001b[39;00m core_exceptions\u001b[39m.\u001b[39mConflict \u001b[39mas\u001b[39;00m create_exc:\n\u001b[1;32m     93\u001b[0m     \u001b[39m# The thought is if someone is providing their own job IDs and they get\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[39m# their job ID generation wrong, this could end up returning results for\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# the wrong query. We thus only try to recover if job ID was not given.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m job_id_given:\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/cloud/bigquery/job/query.py:1310\u001b[0m, in \u001b[0;36mQueryJob._begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"API call:  begin the job via a POST request\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \n\u001b[1;32m   1292\u001b[0m \u001b[39mSee\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[39m    ValueError: If the job has already begun.\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1310\u001b[0m     \u001b[39msuper\u001b[39m(QueryJob, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_begin(client\u001b[39m=\u001b[39mclient, retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m   1311\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mGoogleAPICallError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m   1312\u001b[0m     exc\u001b[39m.\u001b[39mmessage \u001b[39m=\u001b[39m _EXCEPTION_FOOTER_TEMPLATE\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1313\u001b[0m         message\u001b[39m=\u001b[39mexc\u001b[39m.\u001b[39mmessage, location\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation, job_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_id\n\u001b[1;32m   1314\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/cloud/bigquery/job/base.py:693\u001b[0m, in \u001b[0;36m_AsyncJob._begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39m# jobs.insert is idempotent because we ensure that every new\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[39m# job has an ID.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m span_attributes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m: path}\n\u001b[0;32m--> 693\u001b[0m api_response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39m_call_api(\n\u001b[1;32m    694\u001b[0m     retry,\n\u001b[1;32m    695\u001b[0m     span_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBigQuery.job.begin\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    696\u001b[0m     span_attributes\u001b[39m=\u001b[39mspan_attributes,\n\u001b[1;32m    697\u001b[0m     job_ref\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    698\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPOST\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    699\u001b[0m     path\u001b[39m=\u001b[39mpath,\n\u001b[1;32m    700\u001b[0m     data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_api_repr(),\n\u001b[1;32m    701\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    702\u001b[0m )\n\u001b[1;32m    703\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_properties(api_response)\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/cloud/bigquery/client.py:816\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[39mif\u001b[39;00m span_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    813\u001b[0m     \u001b[39mwith\u001b[39;00m create_span(\n\u001b[1;32m    814\u001b[0m         name\u001b[39m=\u001b[39mspan_name, attributes\u001b[39m=\u001b[39mspan_attributes, client\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, job_ref\u001b[39m=\u001b[39mjob_ref\n\u001b[1;32m    815\u001b[0m     ):\n\u001b[0;32m--> 816\u001b[0m         \u001b[39mreturn\u001b[39;00m call()\n\u001b[1;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m call()\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/api_core/retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m target \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    350\u001b[0m     target,\n\u001b[1;32m    351\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predicate,\n\u001b[1;32m    352\u001b[0m     sleep_generator,\n\u001b[1;32m    353\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout,\n\u001b[1;32m    354\u001b[0m     on_error\u001b[39m=\u001b[39mon_error,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/api_core/retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mfor\u001b[39;00m sleep \u001b[39min\u001b[39;00m sleep_generator:\n\u001b[1;32m    190\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m         \u001b[39mreturn\u001b[39;00m target()\n\u001b[1;32m    193\u001b[0m     \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/site-packages/google/cloud/_http/__init__.py:479\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39m# Making the executive decision that any dictionary\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39m# data will be sent properly as JSON.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 479\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(data)\n\u001b[1;32m    480\u001b[0m     content_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[39m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m skipkeys \u001b[39mand\u001b[39;00m ensure_ascii \u001b[39mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[39mand\u001b[39;00m allow_nan \u001b[39mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m indent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m separators \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sort_keys \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_encoder\u001b[39m.\u001b[39mencode(obj)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/json/encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    197\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterencode(o, _one_shot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    202\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/anaconda3/envs/rnalab/lib/python3.11/json/encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    255\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT MAX(updated)\n",
    "    FROM nih-sra-datastore.sra_tax_analysis_tool.tax_analysis\n",
    "    WHERE acc in @existing_runs\n",
    "    ORDER BY acc\n",
    "\"\"\"\n",
    "\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    query_parameters=[\n",
    "        bigquery.ArrayQueryParameter(\"existing_runs\", \"STRING\", existing_stat_runs),\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_job = client.query(query, job_config=job_config) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query data:\n",
      "name=DRR000013, count=Row(('DRR000013', 28734, 'order', 'Macroscelidea', 405, 0, 22, 97, 101), {'acc': 0, 'tax_id': 1, 'rank': 2, 'name': 3, 'total_count': 4, 'self_count': 5, 'ilevel': 6, 'ileft': 7, 'iright': 8})\n",
      "name=DRR000013, count=Row(('DRR000013', 9989, 'order', 'Rodentia', 527, 0, 24, 60, 77), {'acc': 0, 'tax_id': 1, 'rank': 2, 'name': 3, 'total_count': 4, 'self_count': 5, 'ilevel': 6, 'ileft': 7, 'iright': 8})\n",
      "name=DRR000013, count=Row(('DRR000013', 9443, 'order', 'Primates', 719326, 29171, 23, 0, 60), {'acc': 0, 'tax_id': 1, 'rank': 2, 'name': 3, 'total_count': 4, 'self_count': 5, 'ilevel': 6, 'ileft': 7, 'iright': 8})\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT * FROM nih-sra-datastore.sra_tax_analysis_tool.tax_analysis AS tax\n",
    "    WHERE rank = 'order'\n",
    "    AND total_count >= 100\n",
    "    ORDER BY acc\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "print(\"The query data:\")\n",
    "for row in query_job:\n",
    "    # Row values can be accessed by field name or index.\n",
    "    print(\"name={}, count={}\".format(row[0], row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnalab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "448a416c49845ffbddc886562179757704d684dae284cc536cfa1eca10a7a7d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
